{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9\n",
    "\n",
    "## Mashable news stories analysis\n",
    "\n",
    "Predicting if a news story is going to be popular\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>...</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>Popular</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://mashable.com/2014/12/10/cia-torture-rep...</td>\n",
       "      <td>28.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>0.732620</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.844262</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.80</td>\n",
       "      <td>-0.487500</td>\n",
       "      <td>-0.60</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://mashable.com/2013/10/18/bitlock-kicksta...</td>\n",
       "      <td>447.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>0.653199</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.815789</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-0.135340</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://mashable.com/2013/07/24/google-glass-po...</td>\n",
       "      <td>533.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.660377</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.775701</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://mashable.com/2013/11/21/these-are-the-m...</td>\n",
       "      <td>413.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>781.0</td>\n",
       "      <td>0.497409</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.677350</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.195701</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.071429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://mashable.com/2014/02/11/parking-ticket-...</td>\n",
       "      <td>331.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.830357</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.55</td>\n",
       "      <td>-0.175000</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  timedelta  \\\n",
       "0  http://mashable.com/2014/12/10/cia-torture-rep...       28.0   \n",
       "1  http://mashable.com/2013/10/18/bitlock-kicksta...      447.0   \n",
       "2  http://mashable.com/2013/07/24/google-glass-po...      533.0   \n",
       "3  http://mashable.com/2013/11/21/these-are-the-m...      413.0   \n",
       "4  http://mashable.com/2014/02/11/parking-ticket-...      331.0   \n",
       "\n",
       "   n_tokens_title  n_tokens_content  n_unique_tokens  n_non_stop_words  \\\n",
       "0             9.0             188.0         0.732620               1.0   \n",
       "1             7.0             297.0         0.653199               1.0   \n",
       "2            11.0             181.0         0.660377               1.0   \n",
       "3            12.0             781.0         0.497409               1.0   \n",
       "4             8.0             177.0         0.685714               1.0   \n",
       "\n",
       "   n_non_stop_unique_tokens  num_hrefs  num_self_hrefs  num_imgs   ...     \\\n",
       "0                  0.844262        5.0             1.0       1.0   ...      \n",
       "1                  0.815789        9.0             4.0       1.0   ...      \n",
       "2                  0.775701        4.0             3.0       1.0   ...      \n",
       "3                  0.677350       10.0             3.0       1.0   ...      \n",
       "4                  0.830357        3.0             2.0       1.0   ...      \n",
       "\n",
       "   min_positive_polarity  max_positive_polarity  avg_negative_polarity  \\\n",
       "0               0.200000                   0.80              -0.487500   \n",
       "1               0.160000                   0.50              -0.135340   \n",
       "2               0.136364                   1.00               0.000000   \n",
       "3               0.100000                   1.00              -0.195701   \n",
       "4               0.100000                   0.55              -0.175000   \n",
       "\n",
       "   min_negative_polarity  max_negative_polarity  title_subjectivity  \\\n",
       "0                  -0.60              -0.250000                 0.9   \n",
       "1                  -0.40              -0.050000                 0.1   \n",
       "2                   0.00               0.000000                 0.3   \n",
       "3                  -0.40              -0.071429                 0.0   \n",
       "4                  -0.25              -0.100000                 0.0   \n",
       "\n",
       "   title_sentiment_polarity  abs_title_subjectivity  \\\n",
       "0                       0.8                     0.4   \n",
       "1                      -0.1                     0.4   \n",
       "2                       1.0                     0.2   \n",
       "3                       0.0                     0.5   \n",
       "4                       0.0                     0.5   \n",
       "\n",
       "   abs_title_sentiment_polarity  Popular  \n",
       "0                           0.8        1  \n",
       "1                           0.1        0  \n",
       "2                           1.0        0  \n",
       "3                           0.0        0  \n",
       "4                           0.0        0  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://raw.githubusercontent.com/albahnsen/PracticalMachineLearningClass/master/datasets/mashable.csv'\n",
    "df = pd.read_csv(url, index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000, 61)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['url', 'Popular'], axis=1)\n",
    "y = df['Popular']\n",
    "y = y.astype('category')\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Popular</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0    count\n",
       "Popular       \n",
       "1          0.5\n",
       "0          0.5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_ = pd.DataFrame(y)\n",
    "popular = pd.crosstab(index=y_['Popular'], columns='count')\n",
    "popular/popular.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=7351)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9.1\n",
    "\n",
    "Estimate a Decision Tree Classifier and a Logistic Regresion\n",
    "\n",
    "Evaluate using the following metrics:\n",
    "* Accuracy\n",
    "* F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "logreg.fit(X_train,y_train)\n",
    "tree.fit(X_train,y_train)\n",
    "\n",
    "y_logreg_pred = logreg.predict(X_test)\n",
    "y_tree_pred = tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg\n",
      "F1 Score: 0.631\n",
      "Accuracy Score : 0.641\n",
      " \n",
      "Tree\n",
      "F1 Score: 0.56\n",
      "Accuracy Score : 0.554\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"LogReg\n",
    "F1 Score: {round(f1_score(y_test,y_logreg_pred),3)}\n",
    "Accuracy Score : {round(accuracy_score(y_test,y_logreg_pred),3)}\"\"\")\n",
    "print(' ')\n",
    "print(f\"\"\"Tree\n",
    "F1 Score: {round(f1_score(y_test,y_tree_pred),3)}\n",
    "Accuracy Score : {round(accuracy_score(y_test,y_tree_pred),3)}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9.2\n",
    "\n",
    "Estimate 300 bagged samples\n",
    "\n",
    "Estimate the following set of classifiers:\n",
    "\n",
    "* 100 Decision Trees where max_depth=None\n",
    "* 100 Decision Trees where max_depth=2\n",
    "* 100 Logistic Regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelos = {\n",
    "    'DTree_None' : DecisionTreeClassifier(max_depth=None),\n",
    "    'DTree_2' : DecisionTreeClassifier(max_depth=2),\n",
    "    'LogReg' : LogisticRegression()\n",
    "}\n",
    "y_pred = pd.DataFrame(columns = modelos.keys(), index = y_test.index)\n",
    "for model in modelos.keys():\n",
    "    breg = BaggingClassifier(modelos[model], n_estimators=100, n_jobs=-1,\n",
    "                            bootstrap=True, oob_score=True, random_state=7351)\n",
    "    breg.fit(X_train, y_train)\n",
    "    y_pred[model] = breg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DTree_None</th>\n",
       "      <th>DTree_2</th>\n",
       "      <th>LogReg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4078</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3819</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3983</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      DTree_None  DTree_2  LogReg\n",
       "4078           0        0       0\n",
       "340            0        1       1\n",
       "1984           0        1       1\n",
       "3819           1        0       0\n",
       "3983           0        1       0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9.3\n",
    "\n",
    "Ensemble using majority voting\n",
    "\n",
    "Evaluate using the following metrics:\n",
    "* Accuracy\n",
    "* F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    DTree_None\n",
      "    F1 Score: 0.648\n",
      "    Accuracy Score : 0.65\n",
      "    \n",
      "\n",
      "    DTree_2\n",
      "    F1 Score: 0.633\n",
      "    Accuracy Score : 0.635\n",
      "    \n",
      "\n",
      "    LogReg\n",
      "    F1 Score: 0.633\n",
      "    Accuracy Score : 0.643\n",
      "    \n",
      "Para el promedio de los tres:\n",
      "F1 Score: 0.534\n",
      "Accuracy Score : 0.617\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in modelos.keys():\n",
    "    print(f'''\n",
    "    {model}\n",
    "    F1 Score: {round(f1_score(y_test,y_pred[model]),3)}\n",
    "    Accuracy Score : {round(accuracy_score(y_test,y_pred[model]),3)}\n",
    "    ''')\n",
    "\n",
    "print(f'''Para el promedio de los tres:\n",
    "F1 Score: {round(metrics.f1_score(y_pred.mean(axis=1).astype(int), y_test),3)}\n",
    "Accuracy Score : {round(metrics.accuracy_score(y_pred.mean(axis=1).astype(int), y_test),3)}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando VotingClassifier de SKLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=dict_items([('DTree_None', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_w...y='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "         flatten_transform=None, n_jobs=-1, voting='hard', weights=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vclass = VotingClassifier(estimators=modelos.items(),\n",
    "                         n_jobs=-1)\n",
    "vclass.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Con el algoritmo de SKLearn\n",
      "F1 Score: 0.635\n",
      "Accuracy Score : 0.638\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"Con el algoritmo de SKLearn\n",
    "F1 Score: {round(f1_score(y_test,vclass.predict(X_test)),3)}\n",
    "Accuracy Score : {round(accuracy_score(y_test,vclass.predict(X_test)),3)}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9.4\n",
    "\n",
    "Estimate te probability as %models that predict positive\n",
    "\n",
    "Modify the probability threshold and select the one that maximizes the F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Popular</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0    count\n",
       "Popular       \n",
       "1          0.5\n",
       "0          0.5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_ = pd.DataFrame(y)\n",
    "popular = pd.crosstab(index=y_['Popular'], columns='count')\n",
    "popular/popular.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = []\n",
    "y_pred = pd.DataFrame(index=y_test.index, columns=modelos.keys())\n",
    "for k in np.arange(0.09,1,0.1):\n",
    "    for model in modelos.keys():\n",
    "        breg = BaggingClassifier(modelos[model], n_estimators=100, n_jobs=-1,\n",
    "                                bootstrap=True,oob_score=True, random_state=7351)\n",
    "        breg.fit(X_train,y_train)\n",
    "        y_pred[model] = (breg.predict_proba(X_test)[:,1] >= k).astype(bool)\n",
    "        f1.append([model,k,round(f1_score(y_test,y_pred[model]),3),round(metrics.accuracy_score(y_pred[model],y_test))])\n",
    "f1 = pd.DataFrame(f1, columns=['Modelo','Prob','F1_Score','Accuracy_Score'])      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mejor F1 Score DTree_None:\n",
      "Modelo            DTree_None\n",
      "Prob                    0.29\n",
      "F1_Score               0.714\n",
      "Accuracy_Score             1\n",
      "Name: 6, dtype: object\n",
      "\n",
      "Mejor F1 Score DTree_2:\n",
      "Modelo            DTree_2\n",
      "Prob                 0.29\n",
      "F1_Score            0.699\n",
      "Accuracy_Score          1\n",
      "Name: 7, dtype: object\n",
      "\n",
      "Mejor F1 Score LogReg:\n",
      "Modelo            LogReg\n",
      "Prob                0.39\n",
      "F1_Score           0.708\n",
      "Accuracy_Score         1\n",
      "Name: 11, dtype: object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'''\n",
    "Mejor F1 Score DTree_None:\n",
    "{f1.loc[f1[f1.Modelo=='DTree_None']['F1_Score'].idxmax()]}\n",
    "\n",
    "Mejor F1 Score DTree_2:\n",
    "{f1.loc[f1[f1.Modelo=='DTree_2']['F1_Score'].idxmax()]}\n",
    "\n",
    "Mejor F1 Score LogReg:\n",
    "{f1.loc[f1[f1.Modelo=='LogReg']['F1_Score'].idxmax()]}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando VotingClassifier de SKLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prob              0.190\n",
       "F1_Score          0.695\n",
       "Accuracy_Score    0.568\n",
       "Name: 1, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1 = []\n",
    "vclass = VotingClassifier(estimators=modelos.items(),\n",
    "                          voting='soft',\n",
    "                         n_jobs=-1)\n",
    "vclass.fit(X_train,y_train)\n",
    "for k in np.arange(0.09,1,0.1):\n",
    "    y_pred = (vclass.predict_proba(X_test)[:,1] >= k).astype(bool)\n",
    "    f1.append([k,round(f1_score(y_test,y_pred),3),round(vclass.score(X_test,y_test),3)])\n",
    "f1 = pd.DataFrame(f1, columns=['Prob','F1_Score','Accuracy_Score'])\n",
    "f1.loc[f1['F1_Score'].idxmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9.5\n",
    "\n",
    "Ensemble using weighted voting using the oob_error\n",
    "\n",
    "Evaluate using the following metrics:\n",
    "* Accuracy\n",
    "* F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelos_2 = {}\n",
    "f1 = []\n",
    "y_pred = pd.DataFrame(index=y_test.index, columns=modelos.keys())\n",
    "for model in modelos.keys():\n",
    "    modelos_2[model] = BaggingClassifier(modelos[model], n_estimators=100,n_jobs=-1,bootstrap=True, oob_score=True, random_state=7351)\n",
    "    modelos_2[model].fit(X_train, y_train)\n",
    "    y_pred[model] = modelos_2[model].predict(X_test)\n",
    "    f1.append([model, metrics.f1_score(y_pred[model], y_test), metrics.accuracy_score(y_pred[model], y_test)])\n",
    "f1=pd.DataFrame(f1, columns=['Modelo', 'F1_Score', 'Accuracy_Score',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_=[]\n",
    "y_pred = pd.DataFrame(index=y_test.index, columns=modelos_2.keys())\n",
    "for model in modelos_2.keys():\n",
    "    errors = np.zeros(modelos_2[model].n_estimators)\n",
    "    y_pred_all_ = np.zeros((X_test.shape[0], modelos_2[model].n_estimators))\n",
    "    for i in range(modelos_2[model].n_estimators):\n",
    "        oob_sample = ~modelos_2[model].estimators_samples_[i]\n",
    "        y_pred_ = modelos_2[model].estimators_[i].predict(X_train.values[oob_sample])\n",
    "        errors[i] = metrics.accuracy_score(y_pred_, y_train.values[oob_sample])\n",
    "        y_pred_all_[:, i] = modelos_2[model].estimators_[i].predict(X_test)\n",
    "    alpha = (1 - errors) / (1 - errors).sum()\n",
    "    y_pred[model] = (np.sum(y_pred_all_ * alpha, axis=1) >= 0.5).astype(np.int)\n",
    "    f1_.append([model, metrics.f1_score(y_pred[model], y_test), metrics.accuracy_score(y_pred[model], y_test)])\n",
    "f1_=pd.DataFrame(f1_, columns=['Modelo', 'F1_Score', 'Accuracy_Score',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1 Score:\n",
      "Modelo            DTree_None\n",
      "F1_Score            0.650699\n",
      "Accuracy_Score          0.65\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(f'''Best F1 Score:\n",
    "{f1_.loc[f1_['F1_Score'].idxmax()]}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando VotingClassifier de SKLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_mean = []\n",
    "for i in modelos_2.keys():\n",
    "    errors = np.zeros(modelos_2[i].n_estimators)\n",
    "    y_pred_all_ = np.zeros((X_test.shape[0], modelos_2[i].n_estimators))\n",
    "    for l in range(modelos_2[i].n_estimators):\n",
    "        oob_sample = ~modelos_2[i].estimators_samples_[l]\n",
    "        y_pred_ = modelos_2[i].estimators_[l].predict(pd.DataFrame(X_train).values[oob_sample])\n",
    "        errors[l] = metrics.accuracy_score(y_pred_, y_train.values[oob_sample])\n",
    "        y_pred_all_[:, l] = modelos_2[i].estimators_[l].predict(X_test)\n",
    "    alpha = (1 - errors) / (1 - errors).sum()\n",
    "    y_pred = (np.sum(y_pred_all_ * alpha, axis=1) >= 0.5).astype(np.int)\n",
    "    y_pred_mean.append([i, y_pred.mean()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4806666666666667, 0.472, 0.45266666666666666]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_mean = pd.DataFrame(y_pred_mean)\n",
    "y_pred_mean[1].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "F1_Score          0.664\n",
       "Accuracy_Score    0.665\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1 = []\n",
    "vclass = VotingClassifier(estimators=modelos_2.items(),\n",
    "                          voting='soft',\n",
    "                         n_jobs=-1,\n",
    "                          weights=y_pred_mean[1].tolist())\n",
    "vclass.fit(X_train,y_train)\n",
    "y_pred = (vclass.predict_proba(X_test)[:,1] >= 0.5).astype(bool)\n",
    "f1.append([round(f1_score(y_test,y_pred),3),round(vclass.score(X_test,y_test),3)])\n",
    "f1 = pd.DataFrame(f1, columns=['F1_Score','Accuracy_Score'])\n",
    "f1.loc[f1['F1_Score'].idxmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9.6\n",
    "\n",
    "Estimate te probability of the weighted voting\n",
    "\n",
    "Modify the probability threshold and select the one that maximizes the F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f1_=[]\n",
    "y_pred = pd.DataFrame(index=y_test.index, columns=modelos_2.keys())\n",
    "for k in np.arange(0.09,1,0.1):\n",
    "    for model in modelos_2.keys():\n",
    "        errors = np.zeros(modelos_2[model].n_estimators)\n",
    "        y_pred_all_ = np.zeros((X_test.shape[0], modelos_2[model].n_estimators))\n",
    "        for i in range(modelos_2[model].n_estimators):\n",
    "            oob_sample = ~modelos_2[model].estimators_samples_[i]\n",
    "            y_pred_ = modelos_2[model].estimators_[i].predict(X_train.values[oob_sample])\n",
    "            errors[i] = metrics.accuracy_score(y_pred_, y_train.values[oob_sample])\n",
    "            y_pred_all_[:, i] = modelos_2[model].estimators_[i].predict(X_test)\n",
    "        alpha = (1 - errors) / (1 - errors).sum()\n",
    "        y_pred[model] = (np.sum(y_pred_all_ * alpha, axis=1) >= k).astype(np.int)\n",
    "        f1_.append([model,k, metrics.f1_score(y_pred[model], y_test), metrics.accuracy_score(y_pred[model], y_test)])\n",
    "f1_=pd.DataFrame(f1_, columns=['Modelo','Prob', 'F1_Score', 'Accuracy_Score',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1 Score:\n",
      "Modelo            DTree_None\n",
      "Prob                    0.29\n",
      "F1_Score            0.712823\n",
      "Accuracy_Score      0.607333\n",
      "Name: 6, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(f'''Best F1 Score:\n",
    "{f1_.loc[f1_['F1_Score'].idxmax()]}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando VotingClassifier de SKLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prob              0.390\n",
       "F1_Score          0.714\n",
       "Accuracy_Score    0.665\n",
       "Name: 3, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1 = []\n",
    "vclass = VotingClassifier(estimators=modelos_2.items(),\n",
    "                          voting='soft',\n",
    "                         n_jobs=-1,\n",
    "                          weights=y_pred_mean[1].tolist())\n",
    "vclass.fit(X_train,y_train)\n",
    "for k in np.arange(0.09,1,0.1):\n",
    "    y_pred = (vclass.predict_proba(X_test)[:,1] >= k).astype(bool)\n",
    "    f1.append([k, round(f1_score(y_test,y_pred),3),round(vclass.score(X_test,y_test),3)])\n",
    "f1 = pd.DataFrame(f1, columns=['Prob','F1_Score','Accuracy_Score'])\n",
    "f1.loc[f1['F1_Score'].idxmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9.7\n",
    "\n",
    "Estimate a logistic regression using as input the estimated classifiers\n",
    "\n",
    "Modify the probability threshold such that maximizes the F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1=[]\n",
    "y_pred = pd.DataFrame(index=y_test.index, columns=modelos_2.keys())\n",
    "for model in modelos_2.keys():\n",
    "    X_test_ = np.zeros((X_test.shape[0], modelos_2[model].n_estimators))\n",
    "    X_train_ = np.zeros((X_train.shape[0], modelos_2[model].n_estimators))\n",
    "    for i in range(modelos_2[model].n_estimators):\n",
    "        X_train_[:, i] = modelos_2[model].estimators_[i].predict(X_train)\n",
    "        X_test_[:, i] = modelos_2[model].estimators_[i].predict(X_test)\n",
    "    logregcv = LogisticRegressionCV(cv=5)\n",
    "    logregcv.fit(X_train_, y_train)\n",
    "    y_pred[model] = logregcv.predict(X_test_)\n",
    "    f1.append([model, metrics.f1_score(y_pred[model], y_test), metrics.accuracy_score(y_pred[model], y_test)])\n",
    "f1=pd.DataFrame(f1, columns=['Model','F1_Score','Accuracy_Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1 Score:\n",
      "Model             DTree_None\n",
      "F1_Score            0.647887\n",
      "Accuracy_Score          0.65\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(f'''Best F1 Score:\n",
    "{f1.loc[f1['F1_Score'].idxmax()]}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando VotingClassifier de SKLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2 = (vclass.predict_proba(X_train)[:,1] >= 0.390).astype(bool)\n",
    "X_test_2 = (vclass.predict_proba(X_test)[:,1] >= 0.390).astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "F1 Score: 0.714\n",
      "Accuracy Score: 0.65\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logregcv.fit(X_train_2.reshape(-1,1),y_train)\n",
    "print(f'''\n",
    "F1 Score: {round(f1_score(logregcv.predict(X_test_2.reshape(-1,1)),y_test),3)}\n",
    "Accuracy Score: {round(metrics.accuracy_score(logregcv.predict(X_test_2.reshape(-1,1)),y_test),3)}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
