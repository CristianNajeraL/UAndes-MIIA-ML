Gradient Boosting Classifier y XGB Classifier
Gradient Boosting Classifier busca aprender de aquellos modelos en los cuales no es muy fuerte (tiene un desempeño menor); este algoritmo hace el boosting como un proceso de optimización de la función de costo. La idea básica, que yace detrás de este algoritmo, es ir adicionando modelos que se encarguen de mejorar el resultado anterior, con la intención de reducir el costo de la función de costo; siendo los errores del modelo el costo del mismo.
En la primera iteración, se trata de ajustar un modelo para todos los datos, luego de este modelo se van adicionando los siguientes que se enfocan principalmente en las áreas donde el aprendizaje del modelo anterior es menor; esto se hace de manera iterativa para mejorar el desempeño del mismo, por tal razón, se espera que después de las iteraciones el modelo se ajuste de mejor manera a los datos.
XGBoost tiene una estructura similar a Gradient Boosting Classifier, sin embargo cuenta con una serie de características particulares que potencian su rendimiento. Dentro de estas encontramos que los árboles de decisión que se crean con este algoritmo varían en cada iteración, tanto en la cantidad de nodos terminales como en el peso de cada una de las ramas. Adicional a esto, aplican extra randomisation parameter para reducir la correlación entre cada uno de los árboles generados, lo que nos permite tener una mejor desempeño para el modelo, ya que entre menos relación tengamos entre los árboles mejor será en ensamble de los mismos. Además, este algoritmo tiende a ser más rápido que Gradient Boosting Classifier.
XGBoost, por la forma en cómo está diseñado, tiene un mayor control sobre el over-fitting del modelo; lo que, a su vez, permite un mejor desempeño de estos modelos frente a los de Gradient Boosting Classifier.
